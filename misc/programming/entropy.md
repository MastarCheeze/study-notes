# Entropy

## Information

Amout of "surprisal" for a particular outcome.

Amount of information gained if a particular event occurs.

I\(E\) = log2\(1 / P\(E\)\)

## Entropy

Average amount of "surprisal" for a particular event.

Measure of information gain in a single random variable.

H\(E\) = P\(E\) • I\(E\) \+ P\(1\-E\) • I\(1\-E\)

[https://youtu.be/b6VdGHSV6qg](https://youtu.be/b6VdGHSV6qg)
